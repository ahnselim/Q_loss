{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eea0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def absmax_quantize(X):\n",
    "    # Calculate scale\n",
    "    scale = 127 / torch.max(torch.abs(X))\n",
    "\n",
    "    # Quantize\n",
    "    X_quant = (scale * X).round()\n",
    "\n",
    "    # Dequantize\n",
    "    X_dequant = X_quant / scale\n",
    "\n",
    "    return X_quant.to(torch.int8), X_dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873e1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caslab/anaconda3/envs/asl/lib/python3.11/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 4,943,257,728 bytes\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = \"your_model_name\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Print model size\n",
    "print(f\"Model size: {model.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c14d6c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights:\n",
      "tensor([[-0.0183,  0.0071,  0.0219,  ..., -0.0070, -0.0089,  0.0149],\n",
      "        [ 0.0112,  0.0593,  0.0630,  ..., -0.0334, -0.0148,  0.0058],\n",
      "        [ 0.0182,  0.0141,  0.0361,  ..., -0.0432, -0.0388, -0.0233],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0289,  0.0801,  ..., -0.0767, -0.0311, -0.0334],\n",
      "        [ 0.0242, -0.0325,  0.0369,  ..., -0.0123, -0.0269, -0.0151],\n",
      "        [-0.0264, -0.0498, -0.0210,  ...,  0.0601,  0.0130, -0.0007]],\n",
      "       device='cuda:0')\n",
      "\n",
      "Absmax quantized weights:\n",
      "tensor([[ -3,   1,   4,  ...,  -1,  -2,   3],\n",
      "        [  2,  11,  11,  ...,  -6,  -3,   1],\n",
      "        [  3,   3,   6,  ...,  -8,  -7,  -4],\n",
      "        ...,\n",
      "        [  5,   5,  14,  ..., -14,  -6,  -6],\n",
      "        [  4,  -6,   7,  ...,  -2,  -5,  -3],\n",
      "        [ -5,  -9,  -4,  ...,  11,   2,   0]], device='cuda:0',\n",
      "       dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "# Extract weights of the first layer\n",
    "weights = model.model.layers[0].self_attn.q_proj.weight.data\n",
    "\n",
    "print(\"Original weights:\")\n",
    "print(weights)\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_abs_quant, _ = absmax_quantize(weights)\n",
    "print(\"\\nAbsmax quantized weights:\")\n",
    "print(weights_abs_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd46a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Layer 0] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., 0., 0., -0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 0] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([ 0.0938,  0.0938,  0.0938, -0.0000, -0.0938], device='cuda:0')\n",
      "\n",
      "[Layer 0] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0100, -0.0000, 0.0100, 0.0000, 0.0100], device='cuda:0')\n",
      "\n",
      "[Layer 0] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., 0., 0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 1] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0.0592,  0.0000, -0.0000, -0.0000, -0.0592], device='cuda:0')\n",
      "\n",
      "[Layer 1] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0.0441,  0.0000,  0.0000, -0.0441,  0.0441], device='cuda:0')\n",
      "\n",
      "[Layer 1] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0.0131, -0.0131,  0.0000,  0.0131,  0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 1] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., -0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 2] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0.0000, 0.0815, -0.0000, 0.0000, 0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 2] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0765, -0.0000, 0.0765, 0.0000, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 2] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0124, -0.0000, -0.0000, -0.0000, 0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 2] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., -0., -0., -0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 3] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., -0., 0., -0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 3] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([ 0.0416,  0.0416, -0.0000, -0.0416,  0.0416], device='cuda:0')\n",
      "\n",
      "[Layer 3] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0.0000, -0.0000, 0.0289, -0.0000, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 3] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([ 0.0000, -0.0000,  0.0354,  0.0000, -0.0354], device='cuda:0')\n",
      "\n",
      "[Layer 4] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0.0000, 0.0000, 0.0000, -0.0000, 0.0541], device='cuda:0')\n",
      "\n",
      "[Layer 4] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0., -0., -0., -0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 4] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([ 0.0000, -0.0354, -0.0000, -0.0177, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 4] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., -0., -0., -0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 5] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., 0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 5] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0., 0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 5] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([ 0.0198, -0.0198, -0.0000,  0.0000,  0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 5] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0.0664, -0.0000, -0.0000, 0.0000, 0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 6] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([ 0.0000,  0.0000,  0.0000,  0.0000, -0.0314], device='cuda:0')\n",
      "\n",
      "[Layer 6] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0., 0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 6] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0., 0., -0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 6] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0.0000, 0.0388, 0.0000, -0.0000, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 7] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., -0., -0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 7] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0., -0., -0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 7] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0.0131,  0.0131,  0.0131, -0.0131, -0.0131], device='cuda:0')\n",
      "\n",
      "[Layer 7] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., -0., -0., -0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 8] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., 0., -0., -0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 8] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0., -0., -0., -0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 8] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0251, 0.0126, 0.0000, 0.0126, 0.0126], device='cuda:0')\n",
      "\n",
      "[Layer 8] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., 0., 0., -0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 9] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., -0., -0., -0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 9] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0.0561, -0.0561, -0.0000, -0.0561,  0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 9] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([ 0.0179, -0.0179,  0.0179,  0.0179, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 9] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., -0., 0., -0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 10] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0566], device='cuda:0')\n",
      "\n",
      "[Layer 10] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0421, 0.0000, 0.0421, 0.0000, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 10] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0.0000, -0.0000, -0.0960,  0.0000,  0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 10] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., -0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 11] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0.0000,  0.0377, -0.0000, -0.0377, -0.0753], device='cuda:0')\n",
      "\n",
      "[Layer 11] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0., 0., 0., -0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 11] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0000, 0.0000, -0.0000, 0.0188, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 11] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., 0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 12] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., 0., -0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 12] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0., 0., -0., -0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 12] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0000, 0.0257, -0.0000, -0.0000, 0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 12] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., 0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 13] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., 0., 0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 13] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0000, -0.0000, -0.0000, 0.0000, 0.0765], device='cuda:0')\n",
      "\n",
      "[Layer 13] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0247, -0.0000, 0.0988, 0.0000, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 13] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., -0., 0., -0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 14] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., 0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 14] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0., 0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 14] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0.0000, -0.0865, -0.0000, -0.0000, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 14] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., -0., 0., -0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 15] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0.0000, -0.0477,  0.0000, -0.0000, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 15] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0., -0., -0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 15] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([ 0.0430, -0.0000, -0.0000, -0.0430,  0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 15] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., -0., -0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def absmax_quantize_4bit(X):\n",
    "    max_q = 7\n",
    "    min_q = -8\n",
    "    scale = max_q / torch.max(torch.abs(X))\n",
    "    X_quant = torch.clamp((scale * X).round(), min_q, max_q)\n",
    "    X_dequant = X_quant / scale\n",
    "    return X_quant.to(torch.int8), X_dequant\n",
    "\n",
    "# Load model\n",
    "model_name = \"your_model_name\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Apply 4-bit quantization to all Q/K/V/O projections in all layers\n",
    "for i, layer in enumerate(model.model.layers):\n",
    "    attn = layer.self_attn\n",
    "    for proj_name in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]:\n",
    "        weight_fp = getattr(attn, proj_name).weight.data.float()\n",
    "        weight_int4, weight_dequant = absmax_quantize_4bit(weight_fp)\n",
    "        \n",
    "        print(f\"\\n[Layer {i}] {proj_name}\")\n",
    "        print(\"Original (FP16):\", weight_fp.shape)\n",
    "        print(\"Quantized (int4):\", weight_int4.shape)\n",
    "        print(\"Dequantized sample:\", weight_dequant.view(-1)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5459bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def absmax_quantize_4bit(X):\n",
    "    max_q = 7\n",
    "    min_q = -8\n",
    "    scale = max_q / torch.max(torch.abs(X))\n",
    "    X_quant = torch.clamp((scale * X).round(), min_q, max_q)\n",
    "    X_dequant = X_quant / scale\n",
    "    return X_quant.to(torch.int8), X_dequant\n",
    "\n",
    "# Store original weights\n",
    "weights_fp = [param.data.clone() for param in model.parameters()]\n",
    "\n",
    "# Create quantized model\n",
    "model_abs = deepcopy(model)\n",
    "\n",
    "# Apply 4-bit quantization to all parameters\n",
    "weights_abs = []\n",
    "with torch.no_grad():\n",
    "    for param in model_abs.parameters():\n",
    "        _, dequantized = absmax_quantize_4bit(param.data.float())\n",
    "        param.data.copy_(dequantized.to(param.dtype))\n",
    "        weights_abs.append(dequantized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da3bf146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ [Full-Precision Model Output]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caslab/anaconda3/envs/asl/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/caslab/anaconda3/envs/asl/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a fun fact about the moon. I‚Äôm going to guess you‚Äôre going to say it‚Äôs the only natural satellite in the solar system. I‚Äôm going to guess you‚Äôre going to say it‚Äôs the only natural satellite in the solar system. I‚Äôm going to guess you‚Äôre going\n",
      "PPL: 50.70\n",
      "\n",
      "üß™ [4-bit Quantized Model Output]\n",
      "Tell me a fun fact about the moon..attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach\n",
      "PPL: 846522.00\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. ÌÖçÏä§Ìä∏ ÏÉùÏÑ± Ìï®Ïàò\n",
    "def generate_text(model, prompt, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# 3. PPL Í≥ÑÏÇ∞ Ìï®Ïàò\n",
    "def compute_ppl(model, prompt):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    return torch.exp(loss).item()\n",
    "\n",
    "# 4. ÌÖåÏä§Ìä∏ Prompt\n",
    "prompt = \"Tell me a fun fact about the moon.\"\n",
    "\n",
    "# 5. Full-Precision Î™®Îç∏ Í≤∞Í≥º\n",
    "print(\"\\nüß™ [Full-Precision Model Output]\")\n",
    "print(generate_text(model, prompt))\n",
    "print(f\"PPL: {compute_ppl(model, prompt):.2f}\")\n",
    "\n",
    "# 6. ÏñëÏûêÌôîÎêú Î™®Îç∏ Í≤∞Í≥º\n",
    "print(\"\\nüß™ [4-bit Quantized Model Output]\")\n",
    "print(generate_text(model_abs, prompt))\n",
    "print(f\"PPL: {compute_ppl(model_abs, prompt):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736a2ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      8\u001b[39m bnb_config = BitsAndBytesConfig(\n\u001b[32m      9\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     10\u001b[39m     bnb_4bit_use_double_quant=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     11\u001b[39m     bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# or \"fp4\"\u001b[39;00m\n\u001b[32m     12\u001b[39m     bnb_4bit_compute_dtype=torch.bfloat16\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 5. ÌÖçÏä§Ìä∏ ÏÉùÏÑ± Ìï®Ïàò\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_text\u001b[39m(model, prompt, max_new_tokens=\u001b[32m50\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/asl/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/asl/lib/python3.11/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/asl/lib/python3.11/site-packages/transformers/modeling_utils.py:4139\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4137\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDeepSpeed Zero-3 is not compatible with passing a `device_map`.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m4139\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4140\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUsing a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4141\u001b[39m         )\n\u001b[32m   4143\u001b[39m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[32m   4144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[31mValueError\u001b[39m: Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# 1. Î™®Îç∏ Ïù¥Î¶Ñ\n",
    "model_name = \"your_model_name\"\n",
    "\n",
    "# 2. BitsAndBytes 4-bit ÏñëÏûêÌôî ÏÑ§Ï†ï\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # or \"fp4\"\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "# 5. ÌÖçÏä§Ìä∏ ÏÉùÏÑ± Ìï®Ïàò\n",
    "def generate_text(model, prompt, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# 6. PPL Í≥ÑÏÇ∞ Ìï®Ïàò\n",
    "def compute_ppl(model, prompt):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    return torch.exp(loss).item()\n",
    "\n",
    "# 7. ÌÖåÏä§Ìä∏ Prompt\n",
    "prompt = \"Tell me a fun fact about the moon.\"\n",
    "\n",
    "print(\"\\nüß™ [4-bit NF4 Quantized Model Output]\")\n",
    "print(generate_text(model, prompt))\n",
    "print(f\"PPL: {compute_ppl(model, prompt):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
