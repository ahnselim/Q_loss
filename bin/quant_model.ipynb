{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eea0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def absmax_quantize(X):\n",
    "    # Calculate scale\n",
    "    scale = 127 / torch.max(torch.abs(X))\n",
    "\n",
    "    # Quantize\n",
    "    X_quant = (scale * X).round()\n",
    "\n",
    "    # Dequantize\n",
    "    X_dequant = X_quant / scale\n",
    "\n",
    "    return X_quant.to(torch.int8), X_dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873e1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caslab/anaconda3/envs/asl/lib/python3.11/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 4,943,257,728 bytes\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = \"your_model_name\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Print model size\n",
    "print(f\"Model size: {model.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c14d6c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights:\n",
      "tensor([[-0.0183,  0.0071,  0.0219,  ..., -0.0070, -0.0089,  0.0149],\n",
      "        [ 0.0112,  0.0593,  0.0630,  ..., -0.0334, -0.0148,  0.0058],\n",
      "        [ 0.0182,  0.0141,  0.0361,  ..., -0.0432, -0.0388, -0.0233],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0289,  0.0801,  ..., -0.0767, -0.0311, -0.0334],\n",
      "        [ 0.0242, -0.0325,  0.0369,  ..., -0.0123, -0.0269, -0.0151],\n",
      "        [-0.0264, -0.0498, -0.0210,  ...,  0.0601,  0.0130, -0.0007]],\n",
      "       device='cuda:0')\n",
      "\n",
      "Absmax quantized weights:\n",
      "tensor([[ -3,   1,   4,  ...,  -1,  -2,   3],\n",
      "        [  2,  11,  11,  ...,  -6,  -3,   1],\n",
      "        [  3,   3,   6,  ...,  -8,  -7,  -4],\n",
      "        ...,\n",
      "        [  5,   5,  14,  ..., -14,  -6,  -6],\n",
      "        [  4,  -6,   7,  ...,  -2,  -5,  -3],\n",
      "        [ -5,  -9,  -4,  ...,  11,   2,   0]], device='cuda:0',\n",
      "       dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "# Extract weights of the first layer\n",
    "weights = model.model.layers[0].self_attn.q_proj.weight.data\n",
    "\n",
    "print(\"Original weights:\")\n",
    "print(weights)\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_abs_quant, _ = absmax_quantize(weights)\n",
    "print(\"\\nAbsmax quantized weights:\")\n",
    "print(weights_abs_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd46a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Layer 0] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., 0., 0., -0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 0] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([ 0.0938,  0.0938,  0.0938, -0.0000, -0.0938], device='cuda:0')\n",
      "\n",
      "[Layer 0] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0100, -0.0000, 0.0100, 0.0000, 0.0100], device='cuda:0')\n",
      "\n",
      "[Layer 0] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., 0., 0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 1] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0.0592,  0.0000, -0.0000, -0.0000, -0.0592], device='cuda:0')\n",
      "\n",
      "[Layer 1] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0.0441,  0.0000,  0.0000, -0.0441,  0.0441], device='cuda:0')\n",
      "\n",
      "[Layer 1] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0.0131, -0.0131,  0.0000,  0.0131,  0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 1] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., -0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 2] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0.0000, 0.0815, -0.0000, 0.0000, 0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 2] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0765, -0.0000, 0.0765, 0.0000, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 2] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0124, -0.0000, -0.0000, -0.0000, 0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 2] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., -0., -0., -0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 3] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., -0., 0., -0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 3] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([ 0.0416,  0.0416, -0.0000, -0.0416,  0.0416], device='cuda:0')\n",
      "\n",
      "[Layer 3] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0.0000, -0.0000, 0.0289, -0.0000, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 3] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([ 0.0000, -0.0000,  0.0354,  0.0000, -0.0354], device='cuda:0')\n",
      "\n",
      "[Layer 4] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0.0000, 0.0000, 0.0000, -0.0000, 0.0541], device='cuda:0')\n",
      "\n",
      "[Layer 4] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0., -0., -0., -0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 4] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([ 0.0000, -0.0354, -0.0000, -0.0177, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 4] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., -0., -0., -0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 5] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., 0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 5] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0., 0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 5] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([ 0.0198, -0.0198, -0.0000,  0.0000,  0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 5] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0.0664, -0.0000, -0.0000, 0.0000, 0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 6] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([ 0.0000,  0.0000,  0.0000,  0.0000, -0.0314], device='cuda:0')\n",
      "\n",
      "[Layer 6] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0., 0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 6] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0., 0., -0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 6] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0.0000, 0.0388, 0.0000, -0.0000, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 7] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., -0., -0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 7] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0., -0., -0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 7] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0.0131,  0.0131,  0.0131, -0.0131, -0.0131], device='cuda:0')\n",
      "\n",
      "[Layer 7] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., -0., -0., -0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 8] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., 0., -0., -0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 8] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0., -0., -0., -0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 8] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0251, 0.0126, 0.0000, 0.0126, 0.0126], device='cuda:0')\n",
      "\n",
      "[Layer 8] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., 0., 0., -0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 9] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., -0., -0., -0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 9] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0.0561, -0.0561, -0.0000, -0.0561,  0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 9] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([ 0.0179, -0.0179,  0.0179,  0.0179, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 9] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., -0., 0., -0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 10] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0566], device='cuda:0')\n",
      "\n",
      "[Layer 10] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0421, 0.0000, 0.0421, 0.0000, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 10] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0.0000, -0.0000, -0.0960,  0.0000,  0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 10] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., -0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 11] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0.0000,  0.0377, -0.0000, -0.0377, -0.0753], device='cuda:0')\n",
      "\n",
      "[Layer 11] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0., 0., 0., -0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 11] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0000, 0.0000, -0.0000, 0.0188, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 11] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., 0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 12] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., 0., -0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 12] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0., 0., -0., -0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 12] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0000, 0.0257, -0.0000, -0.0000, 0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 12] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., 0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 13] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0., 0., 0., 0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 13] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0000, -0.0000, -0.0000, 0.0000, 0.0765], device='cuda:0')\n",
      "\n",
      "[Layer 13] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([0.0247, -0.0000, 0.0988, 0.0000, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 13] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., -0., 0., -0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 14] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., 0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 14] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0., 0., 0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 14] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0.0000, -0.0865, -0.0000, -0.0000, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 14] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., -0., 0., -0., 0.], device='cuda:0')\n",
      "\n",
      "[Layer 15] q_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([-0.0000, -0.0477,  0.0000, -0.0000, -0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 15] k_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([-0., -0., -0., 0., -0.], device='cuda:0')\n",
      "\n",
      "[Layer 15] v_proj\n",
      "Original (FP16): torch.Size([512, 2048])\n",
      "Quantized (int4): torch.Size([512, 2048])\n",
      "Dequantized sample: tensor([ 0.0430, -0.0000, -0.0000, -0.0430,  0.0000], device='cuda:0')\n",
      "\n",
      "[Layer 15] o_proj\n",
      "Original (FP16): torch.Size([2048, 2048])\n",
      "Quantized (int4): torch.Size([2048, 2048])\n",
      "Dequantized sample: tensor([0., -0., -0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def absmax_quantize_4bit(X):\n",
    "    max_q = 7\n",
    "    min_q = -8\n",
    "    scale = max_q / torch.max(torch.abs(X))\n",
    "    X_quant = torch.clamp((scale * X).round(), min_q, max_q)\n",
    "    X_dequant = X_quant / scale\n",
    "    return X_quant.to(torch.int8), X_dequant\n",
    "\n",
    "# Load model\n",
    "model_name = \"your_model_name\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Apply 4-bit quantization to all Q/K/V/O projections in all layers\n",
    "for i, layer in enumerate(model.model.layers):\n",
    "    attn = layer.self_attn\n",
    "    for proj_name in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]:\n",
    "        weight_fp = getattr(attn, proj_name).weight.data.float()\n",
    "        weight_int4, weight_dequant = absmax_quantize_4bit(weight_fp)\n",
    "        \n",
    "        print(f\"\\n[Layer {i}] {proj_name}\")\n",
    "        print(\"Original (FP16):\", weight_fp.shape)\n",
    "        print(\"Quantized (int4):\", weight_int4.shape)\n",
    "        print(\"Dequantized sample:\", weight_dequant.view(-1)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5459bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def absmax_quantize_4bit(X):\n",
    "    max_q = 7\n",
    "    min_q = -8\n",
    "    scale = max_q / torch.max(torch.abs(X))\n",
    "    X_quant = torch.clamp((scale * X).round(), min_q, max_q)\n",
    "    X_dequant = X_quant / scale\n",
    "    return X_quant.to(torch.int8), X_dequant\n",
    "\n",
    "# Store original weights\n",
    "weights_fp = [param.data.clone() for param in model.parameters()]\n",
    "\n",
    "# Create quantized model\n",
    "model_abs = deepcopy(model)\n",
    "\n",
    "# Apply 4-bit quantization to all parameters\n",
    "weights_abs = []\n",
    "with torch.no_grad():\n",
    "    for param in model_abs.parameters():\n",
    "        _, dequantized = absmax_quantize_4bit(param.data.float())\n",
    "        param.data.copy_(dequantized.to(param.dtype))\n",
    "        weights_abs.append(dequantized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da3bf146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 [Full-Precision Model Output]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caslab/anaconda3/envs/asl/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/caslab/anaconda3/envs/asl/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a fun fact about the moon. I’m going to guess you’re going to say it’s the only natural satellite in the solar system. I’m going to guess you’re going to say it’s the only natural satellite in the solar system. I’m going to guess you’re going\n",
      "PPL: 50.70\n",
      "\n",
      "🧪 [4-bit Quantized Model Output]\n",
      "Tell me a fun fact about the moon..attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach.attach\n",
      "PPL: 846522.00\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. 텍스트 생성 함수\n",
    "def generate_text(model, prompt, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# 3. PPL 계산 함수\n",
    "def compute_ppl(model, prompt):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    return torch.exp(loss).item()\n",
    "\n",
    "# 4. 테스트 Prompt\n",
    "prompt = \"Tell me a fun fact about the moon.\"\n",
    "\n",
    "# 5. Full-Precision 모델 결과\n",
    "print(\"\\n🧪 [Full-Precision Model Output]\")\n",
    "print(generate_text(model, prompt))\n",
    "print(f\"PPL: {compute_ppl(model, prompt):.2f}\")\n",
    "\n",
    "# 6. 양자화된 모델 결과\n",
    "print(\"\\n🧪 [4-bit Quantized Model Output]\")\n",
    "print(generate_text(model_abs, prompt))\n",
    "print(f\"PPL: {compute_ppl(model_abs, prompt):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736a2ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      8\u001b[39m bnb_config = BitsAndBytesConfig(\n\u001b[32m      9\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     10\u001b[39m     bnb_4bit_use_double_quant=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     11\u001b[39m     bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# or \"fp4\"\u001b[39;00m\n\u001b[32m     12\u001b[39m     bnb_4bit_compute_dtype=torch.bfloat16\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 5. 텍스트 생성 함수\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_text\u001b[39m(model, prompt, max_new_tokens=\u001b[32m50\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/asl/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/asl/lib/python3.11/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/asl/lib/python3.11/site-packages/transformers/modeling_utils.py:4139\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4137\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDeepSpeed Zero-3 is not compatible with passing a `device_map`.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m4139\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4140\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUsing a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4141\u001b[39m         )\n\u001b[32m   4143\u001b[39m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[32m   4144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[31mValueError\u001b[39m: Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# 1. 모델 이름\n",
    "model_name = \"your_model_name\"\n",
    "\n",
    "# 2. BitsAndBytes 4-bit 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # or \"fp4\"\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "# 5. 텍스트 생성 함수\n",
    "def generate_text(model, prompt, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# 6. PPL 계산 함수\n",
    "def compute_ppl(model, prompt):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    return torch.exp(loss).item()\n",
    "\n",
    "# 7. 테스트 Prompt\n",
    "prompt = \"Tell me a fun fact about the moon.\"\n",
    "\n",
    "print(\"\\n🧪 [4-bit NF4 Quantized Model Output]\")\n",
    "print(generate_text(model, prompt))\n",
    "print(f\"PPL: {compute_ppl(model, prompt):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
