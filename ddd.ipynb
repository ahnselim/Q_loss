{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f9eeebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "903f75d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def absmax_quantize(X):\n",
    "    # Calculate scale\n",
    "    scale = 127 / torch.max(torch.abs(X))\n",
    "\n",
    "    # Quantize\n",
    "    X_quant = (scale * X).round()\n",
    "\n",
    "    # Dequantize\n",
    "    X_dequant = X_quant / scale\n",
    "\n",
    "    return X_quant.to(torch.int8), X_dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "991a70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeropoint_quantize(X):\n",
    "    # Calculate value range (denominator)\n",
    "    x_range = torch.max(X) - torch.min(X)\n",
    "    x_range = 1 if x_range == 0 else x_range\n",
    "\n",
    "    # Calculate scale\n",
    "    scale = 255 / x_range\n",
    "\n",
    "    # Shift by zero-point\n",
    "    zeropoint = (-scale * torch.min(X) - 128).round()\n",
    "\n",
    "    # Scale and round the inputs\n",
    "    X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n",
    "\n",
    "    # Dequantize\n",
    "    X_dequant = (X_quant - zeropoint) / scale\n",
    "\n",
    "    return X_quant.to(torch.int8), X_dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81448229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 4,943,257,728 bytes\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(f'cuda:{1}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) \n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = \"your model\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Print model size\n",
    "print(f\"Model size: {model.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fc48a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights:\n",
      "Parameter containing:\n",
      "tensor([[-0.0183,  0.0071,  0.0219,  ..., -0.0070, -0.0089,  0.0149],\n",
      "        [ 0.0112,  0.0593,  0.0630,  ..., -0.0334, -0.0148,  0.0058],\n",
      "        [ 0.0182,  0.0141,  0.0361,  ..., -0.0432, -0.0388, -0.0233],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0289,  0.0801,  ..., -0.0767, -0.0311, -0.0334],\n",
      "        [ 0.0242, -0.0325,  0.0369,  ..., -0.0123, -0.0269, -0.0151],\n",
      "        [-0.0264, -0.0498, -0.0210,  ...,  0.0601,  0.0130, -0.0007]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "torch.Size([2048, 2048])\n",
      "\n",
      "Absmax quantized weights:\n",
      "tensor([[ -3,   1,   4,  ...,  -1,  -2,   3],\n",
      "        [  2,  11,  11,  ...,  -6,  -3,   1],\n",
      "        [  3,   3,   6,  ...,  -8,  -7,  -4],\n",
      "        ...,\n",
      "        [  5,   5,  14,  ..., -14,  -6,  -6],\n",
      "        [  4,  -6,   7,  ...,  -2,  -5,  -3],\n",
      "        [ -5,  -9,  -4,  ...,  11,   2,   0]], device='cuda:1',\n",
      "       dtype=torch.int8)\n",
      "torch.Size([2048, 2048])\n",
      "\n",
      "Zero-point quantized weights:\n",
      "tensor([[ 6, 11, 14,  ...,  9,  8, 13],\n",
      "        [12, 22, 22,  ...,  3,  7, 11],\n",
      "        [14, 13, 17,  ...,  2,  2,  5],\n",
      "        ...,\n",
      "        [16, 16, 26,  ..., -5,  4,  3],\n",
      "        [15,  4, 17,  ...,  8,  5,  7],\n",
      "        [ 5,  0,  6,  ..., 22, 13, 10]], device='cuda:1', dtype=torch.int8)\n",
      "torch.Size([2048, 2048])\n"
     ]
    }
   ],
   "source": [
    "# Extract weights of the first layer\n",
    "weights = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Original weights:\")\n",
    "print(weights)\n",
    "print(weights.size())\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_abs_quant, _ = absmax_quantize(weights)\n",
    "print(\"\\nAbsmax quantized weights:\")\n",
    "print(weights_abs_quant)\n",
    "print(weights_abs_quant.size())\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_zp_quant, _ = zeropoint_quantize(weights)\n",
    "print(\"\\nZero-point quantized weights:\")\n",
    "print(weights_zp_quant)\n",
    "print(weights_zp_quant.size())\n",
    "\n",
    "# print(\"\\nminus :\")\n",
    "# minus=torch.sub(weights,weights_abs_quant)\n",
    "# print(minus)\n",
    "# print(minus.size())\n",
    "\n",
    "# print(\"\\n(W-W_q)+W_q\")\n",
    "# plus=torch.add(minus,weights_abs_quant)\n",
    "# print(plus)\n",
    "# print(plus.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b0af3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 quantization MSE: 0.000003\n",
      "보정 후 MSE (Low-rank 16): 0.000003\n",
      "fp_weight.shape     = torch.Size([2048, 2048])\n",
      "W_restored.shape    = torch.Size([2048, 2048])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def absmax_quantize(weight):\n",
    "    max_val = weight.abs().max()\n",
    "    scale = max_val / 127\n",
    "    quant = torch.round(weight / scale).clamp(-127, 127)\n",
    "    dequant = quant * scale\n",
    "    return dequant, scale\n",
    "\n",
    "def low_rank_restore(fp_weight, quant_weight, rank=8):\n",
    "    if fp_weight.numel() < 2 or quant_weight.numel() < 2:\n",
    "        raise ValueError(\"Too small tensor for SVD\")\n",
    "\n",
    "    original_shape = fp_weight.shape\n",
    "    fp_matrix = fp_weight.view(original_shape[0], -1)\n",
    "    quant_matrix = quant_weight.view(original_shape[0], -1)\n",
    "\n",
    "    delta = fp_matrix - quant_matrix\n",
    "    U, S, Vh = torch.linalg.svd(delta, full_matrices=False)\n",
    "    U_r = U[:, :rank]\n",
    "    S_r = torch.diag(S[:rank])\n",
    "    V_r = Vh[:rank, :]\n",
    "    delta_approx = U_r @ S_r @ V_r\n",
    "    W_approx = quant_matrix + delta_approx\n",
    "\n",
    "    return W_approx.view(original_shape), delta_approx.view(original_shape)\n",
    "\n",
    "\n",
    "# 1. Extract original weight\n",
    "fp_weight = model.model.layers[0].self_attn.q_proj.weight.detach().cpu()\n",
    "\n",
    "# 2. Quantize\n",
    "quant_weight, scale_abs = absmax_quantize(fp_weight)\n",
    "\n",
    "# 3. Restore\n",
    "W_restored, delta_approx = low_rank_restore(fp_weight, quant_weight, rank=16)\n",
    "\n",
    "# 4. Compare MSE\n",
    "mse_original = torch.mean((fp_weight - quant_weight) ** 2).item()\n",
    "mse_recovered = torch.mean((fp_weight - W_restored) ** 2).item()\n",
    "\n",
    "print(f\"원래 quantization MSE: {mse_original:.6f}\")\n",
    "print(f\"보정 후 MSE (Low-rank {16}): {mse_recovered:.6f}\")\n",
    "\n",
    "# 5. Shape 확인\n",
    "print(f\"fp_weight.shape     = {fp_weight.shape}\")\n",
    "print(f\"W_restored.shape    = {W_restored.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41494d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] Layer 0: Too small tensor for SVD\n",
      "[Error] Layer 1: Too small tensor for SVD\n",
      "[Error] Layer 2: Too small tensor for SVD\n",
      "[Error] Layer 3: Too small tensor for SVD\n",
      "[Error] Layer 4: Too small tensor for SVD\n",
      "[Error] Layer 5: Too small tensor for SVD\n",
      "[Error] Layer 6: Too small tensor for SVD\n",
      "[Error] Layer 7: Too small tensor for SVD\n",
      "[Error] Layer 8: Too small tensor for SVD\n",
      "[Error] Layer 9: Too small tensor for SVD\n",
      "[Error] Layer 10: Too small tensor for SVD\n",
      "[Error] Layer 11: Too small tensor for SVD\n",
      "[Error] Layer 12: Too small tensor for SVD\n",
      "[Error] Layer 13: Too small tensor for SVD\n",
      "[Error] Layer 14: Too small tensor for SVD\n",
      "[Error] Layer 15: Too small tensor for SVD\n",
      "[Error] Layer 16: Too small tensor for SVD\n",
      "[Error] Layer 17: Too small tensor for SVD\n",
      "[Error] Layer 18: Too small tensor for SVD\n",
      "[Error] Layer 19: Too small tensor for SVD\n",
      "[Error] Layer 20: Too small tensor for SVD\n",
      "[Error] Layer 21: Too small tensor for SVD\n",
      "[Error] Layer 22: Too small tensor for SVD\n",
      "[Error] Layer 23: Too small tensor for SVD\n",
      "[Error] Layer 24: Too small tensor for SVD\n",
      "[Error] Layer 25: Too small tensor for SVD\n",
      "[Error] Layer 26: Too small tensor for SVD\n",
      "[Error] Layer 27: Too small tensor for SVD\n",
      "[Error] Layer 28: Too small tensor for SVD\n",
      "[Error] Layer 29: Too small tensor for SVD\n",
      "[Error] Layer 30: Too small tensor for SVD\n",
      "[Error] Layer 31: Too small tensor for SVD\n",
      "[Error] Layer 32: Too small tensor for SVD\n",
      "[Error] Layer 33: Too small tensor for SVD\n",
      "[Error] Layer 34: Too small tensor for SVD\n",
      "[Error] Layer 35: Too small tensor for SVD\n",
      "[Error] Layer 36: Too small tensor for SVD\n",
      "[Error] Layer 37: Too small tensor for SVD\n",
      "[Error] Layer 38: Too small tensor for SVD\n",
      "[Error] Layer 39: Too small tensor for SVD\n",
      "[Error] Layer 40: Too small tensor for SVD\n",
      "[Error] Layer 41: Too small tensor for SVD\n",
      "[Error] Layer 42: Too small tensor for SVD\n",
      "[Error] Layer 43: Too small tensor for SVD\n",
      "[Error] Layer 44: Too small tensor for SVD\n",
      "[Error] Layer 45: Too small tensor for SVD\n",
      "[Error] Layer 46: Too small tensor for SVD\n",
      "[Error] Layer 47: Too small tensor for SVD\n",
      "[Error] Layer 48: Too small tensor for SVD\n",
      "[Error] Layer 49: Too small tensor for SVD\n",
      "[Error] Layer 50: Too small tensor for SVD\n",
      "[Error] Layer 51: Too small tensor for SVD\n",
      "[Error] Layer 52: Too small tensor for SVD\n",
      "[Error] Layer 53: Too small tensor for SVD\n",
      "[Error] Layer 54: Too small tensor for SVD\n",
      "[Error] Layer 55: Too small tensor for SVD\n",
      "[Error] Layer 56: Too small tensor for SVD\n",
      "[Error] Layer 57: Too small tensor for SVD\n",
      "[Error] Layer 58: Too small tensor for SVD\n",
      "[Error] Layer 59: Too small tensor for SVD\n",
      "[Error] Layer 60: Too small tensor for SVD\n",
      "[Error] Layer 61: Too small tensor for SVD\n",
      "[Error] Layer 62: Too small tensor for SVD\n",
      "[Error] Layer 63: Too small tensor for SVD\n",
      "[Error] Layer 64: Too small tensor for SVD\n",
      "[Error] Layer 65: Too small tensor for SVD\n",
      "[Error] Layer 66: Too small tensor for SVD\n",
      "[Error] Layer 67: Too small tensor for SVD\n",
      "[Error] Layer 68: Too small tensor for SVD\n",
      "[Error] Layer 69: Too small tensor for SVD\n",
      "[Error] Layer 70: Too small tensor for SVD\n",
      "[Error] Layer 71: Too small tensor for SVD\n",
      "[Error] Layer 72: Too small tensor for SVD\n",
      "[Error] Layer 73: Too small tensor for SVD\n",
      "[Error] Layer 74: Too small tensor for SVD\n",
      "[Error] Layer 75: Too small tensor for SVD\n",
      "[Error] Layer 76: Too small tensor for SVD\n",
      "[Error] Layer 77: Too small tensor for SVD\n",
      "[Error] Layer 78: Too small tensor for SVD\n",
      "[Error] Layer 79: Too small tensor for SVD\n",
      "[Error] Layer 80: Too small tensor for SVD\n",
      "[Error] Layer 81: Too small tensor for SVD\n",
      "[Error] Layer 82: Too small tensor for SVD\n",
      "[Error] Layer 83: Too small tensor for SVD\n",
      "[Error] Layer 84: Too small tensor for SVD\n",
      "[Error] Layer 85: Too small tensor for SVD\n",
      "[Error] Layer 86: Too small tensor for SVD\n",
      "[Error] Layer 87: Too small tensor for SVD\n",
      "[Error] Layer 88: Too small tensor for SVD\n",
      "[Error] Layer 89: Too small tensor for SVD\n",
      "[Error] Layer 90: Too small tensor for SVD\n",
      "[Error] Layer 91: Too small tensor for SVD\n",
      "[Error] Layer 92: Too small tensor for SVD\n",
      "[Error] Layer 93: Too small tensor for SVD\n",
      "[Error] Layer 94: Too small tensor for SVD\n",
      "[Error] Layer 95: Too small tensor for SVD\n",
      "[Error] Layer 96: Too small tensor for SVD\n",
      "[Error] Layer 97: Too small tensor for SVD\n",
      "[Error] Layer 98: Too small tensor for SVD\n",
      "[Error] Layer 99: Too small tensor for SVD\n",
      "[Error] Layer 100: Too small tensor for SVD\n",
      "[Error] Layer 101: Too small tensor for SVD\n",
      "[Error] Layer 102: Too small tensor for SVD\n",
      "[Error] Layer 103: Too small tensor for SVD\n",
      "[Error] Layer 104: Too small tensor for SVD\n",
      "[Error] Layer 105: Too small tensor for SVD\n",
      "[Error] Layer 106: Too small tensor for SVD\n",
      "[Error] Layer 107: Too small tensor for SVD\n",
      "[Error] Layer 108: Too small tensor for SVD\n",
      "[Error] Layer 109: Too small tensor for SVD\n",
      "[Error] Layer 110: Too small tensor for SVD\n",
      "[Error] Layer 111: Too small tensor for SVD\n",
      "[Error] Layer 112: Too small tensor for SVD\n",
      "[Error] Layer 113: Too small tensor for SVD\n",
      "[Error] Layer 114: Too small tensor for SVD\n",
      "[Error] Layer 115: Too small tensor for SVD\n",
      "[Error] Layer 116: Too small tensor for SVD\n",
      "[Error] Layer 117: Too small tensor for SVD\n",
      "[Error] Layer 118: Too small tensor for SVD\n",
      "[Error] Layer 119: Too small tensor for SVD\n",
      "[Error] Layer 120: Too small tensor for SVD\n",
      "[Error] Layer 121: Too small tensor for SVD\n",
      "[Error] Layer 122: Too small tensor for SVD\n",
      "[Error] Layer 123: Too small tensor for SVD\n",
      "[Error] Layer 124: Too small tensor for SVD\n",
      "[Error] Layer 125: Too small tensor for SVD\n",
      "[Error] Layer 126: Too small tensor for SVD\n",
      "[Error] Layer 127: Too small tensor for SVD\n",
      "[Error] Layer 128: Too small tensor for SVD\n",
      "[Error] Layer 129: Too small tensor for SVD\n",
      "[Error] Layer 130: Too small tensor for SVD\n",
      "[Error] Layer 131: Too small tensor for SVD\n",
      "[Error] Layer 132: Too small tensor for SVD\n",
      "[Error] Layer 133: Too small tensor for SVD\n",
      "[Error] Layer 134: Too small tensor for SVD\n",
      "[Error] Layer 135: Too small tensor for SVD\n",
      "[Error] Layer 136: Too small tensor for SVD\n",
      "[Error] Layer 137: Too small tensor for SVD\n",
      "[Error] Layer 138: Too small tensor for SVD\n",
      "[Error] Layer 139: Too small tensor for SVD\n",
      "[Error] Layer 140: Too small tensor for SVD\n",
      "[Error] Layer 141: Too small tensor for SVD\n",
      "[Error] Layer 142: Too small tensor for SVD\n",
      "[Error] Layer 143: Too small tensor for SVD\n",
      "[Error] Layer 144: Too small tensor for SVD\n",
      "[Error] Layer 145: Too small tensor for SVD\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "# ---------- Quantization Functions ----------\n",
    "def absmax_quantize(weight):\n",
    "    max_val = weight.abs().max()\n",
    "    scale = max_val / 127\n",
    "    quant = torch.round(weight / scale).clamp(-127, 127)\n",
    "    dequant = quant * scale\n",
    "    return dequant, scale\n",
    "\n",
    "# 0\n",
    "\n",
    "def low_rank_restore(fp_weight, quant_weight, rank=8):\n",
    "    if fp_weight.numel() < 2 or quant_weight.numel() < 2:\n",
    "        raise ValueError(\"Too small tensor for SVD\")\n",
    "\n",
    "    original_shape = fp_weight.shape\n",
    "    fp_matrix = fp_weight.view(original_shape[0], -1)\n",
    "    quant_matrix = quant_weight.view(original_shape[0], -1)\n",
    "\n",
    "    delta = fp_matrix - quant_matrix\n",
    "    U, S, Vh = torch.linalg.svd(delta, full_matrices=False)\n",
    "    U_r = U[:, :rank]\n",
    "    S_r = torch.diag(S[:rank])\n",
    "    V_r = Vh[:rank, :]\n",
    "    delta_approx = U_r @ S_r @ V_r\n",
    "    W_approx = quant_matrix + delta_approx\n",
    "\n",
    "    return W_approx.view(original_shape), delta_approx.view(original_shape)\n",
    "\n",
    "\n",
    "# ---------- Step 1: 원본 weight 저장 ----------\n",
    "# 1. Extract original weight\n",
    "fp_weight = [param.data.cpu().clone() for param in model.parameters()]\n",
    "\n",
    "# ---------- Step 2: absmax 양자화 모델 ----------\n",
    "model_abs = deepcopy(model)\n",
    "weights_abs = []\n",
    "\n",
    "for param in model_abs.parameters():\n",
    "    _, dequantized = absmax_quantize(param.data)\n",
    "    param.data = dequantized\n",
    "    weights_abs.append(dequantized)\n",
    "\n",
    "# # ---------- Step 3: zero-point 양자화 모델 ----------\n",
    "# model_zp = deepcopy(model)\n",
    "# weights_zp = []\n",
    "\n",
    "# for param in model_zp.parameters():\n",
    "#     _, dequant = zeropoint_quantize(param.data.cpu())\n",
    "#     param.data.copy_(dequant.to(dtype=param.data.dtype))\n",
    "#     weights_zp.append(dequant.clone())\n",
    "\n",
    "# ---------- Step 4: low-rank 보정 모델 ----------\n",
    "model_restored = deepcopy(model)\n",
    "weights_restored=[]\n",
    "\n",
    "for i, param in enumerate(model_abs.parameters()):\n",
    "    fp = fp_weight[i]        # 저장된 full-precision weight\n",
    "    quant = weights_abs[i]    # 저장된 absmax quantized weight\n",
    "\n",
    "    try:\n",
    "        restored, _ = low_rank_restore(fp, quant, rank=8)\n",
    "        param.data.copy_(restored.to(dtype=param.data.dtype))\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Layer {i}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6cbee73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param.shape = torch.Size([128256, 2048])\n",
      "restored.shape = torch.Size([2048, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(f\"param.shape = {param.data.shape}\")\n",
    "print(f\"restored.shape = {restored.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19b35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      "I have a dream that one day the people of the world will live together in a world without race prejudice. I have a dream that one day, the sons of former slaves and the sons of former slave owners will be able to sit down together\n",
      "--------------------------------------------------\n",
      "Absmax model:\n",
      "I have a dream that one day every child will be able to sleep in a safe, secure, and comfortable bed, that every child will have a chance to grow up in a safe, secure, and comfortable home, that every child will have\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def generate_text(model, input_text, max_length=50):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    output = model.generate(inputs=input_ids,\n",
    "                            max_length=max_length,\n",
    "                            do_sample=True,\n",
    "                            top_k=30,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            attention_mask=input_ids.new_ones(input_ids.shape))\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate text with original and quantized models\n",
    "original_text = generate_text(model, \"I have a dream\")\n",
    "absmax_text   = generate_text(model_abs, \"I have a dream\")\n",
    "# zp_text       = generate_text(model_zp, \"I have a dream\")\n",
    "\n",
    "print(f\"Original model:\\n{original_text}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Absmax model:\\n{absmax_text}\")\n",
    "print(\"-\" * 50)\n",
    "# print(f\"Zeropoint model:\\n{zp_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
